{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitNativ1984/Autobrains-Unet-Cityscapes/blob/main/Algo_DNN_Home_Assignment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJm1RJJykGMu"
      },
      "source": [
        "## TASK 1 - Semantic Segmentation using U-net\n",
        "\n",
        "Identifying the drivable area provides critical information for navigating and path planning in autonomous driving. In this task you will take the first step torward this feature by segmenting the road in the image.\n",
        "\n",
        "\n",
        "# **Save Your Time**\n",
        "Please first download the data. The data is transfered via Google drive which somtimes slow.\n",
        "*   [Zip file - containing the data](https://drive.google.com/file/d/1_Demk2hTXuVPq9bBToedS4fOLqwaG7N3/view?usp=share_link)\n",
        "*   [Google drive with all the files](https://drive.google.com/drive/folders/10udfdPAT0yPq1TA7Ld0ddSKMedbEJBoe?usp=share_link)\n",
        "\n",
        "![](https://raw.githubusercontent.com/henyau/Image-Segmentation-with-Unet/master/images/train_label.png)\n",
        "\n",
        "\n",
        "You are provided with a partial training code of a U-net model. Your task is to train a model that predicts the road segment in the image. Please implement the missing components of the training pipeline, train the model and preform evaluation.\n",
        "\n",
        "### Data - road and vehicle segmentation\n",
        "\n",
        "The zip file provided contains images and high quality dense pixel annotations (fine annotation). The data is split into train (\\~2900 images) and validation (\\~500 images) with the directories similar the original Cityscapes dataset. \n",
        "\n",
        "The original Cityscapes anotations contains 19 classes. Please reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”.You can read more about Cityscape here https://www.cityscapes-dataset.com/dataset-overview/#labeling-policy.\n",
        "\n",
        "\n",
        "The Zip file provides with the following:\n",
        "1.   cityscapes dir - a directory with the images and annotations. Make sure the annotations contains the 3 classes mentioned above\n",
        "2.   image_list dir - a tsv file for train and validation. Each row in the tsv is a pairs of paths, path to image and path to annotation. In each TSV change **/PATH_TO_CITYSCAPES_DATA_DIR/** to the absolute path of cityscapes dir\n",
        "\n",
        "\n",
        "\n",
        "To save time:\n",
        "*   Resize images to a small fixed-size that will enable you to train the model in a fesiable time.\n",
        "\n",
        "# **Deliverables**\n",
        "\n",
        "\n",
        "1.   Complete the training code for road segmentation (marked \"COMPLETE ME\"):\n",
        "  * **Loss function** (Training Flow section)\n",
        "  * **Optimizer** (Training Flow section)\n",
        "  * **Augmentations** (Data preparations section)\n",
        "2.   **Perform an evaluation of your trained model** (Evaluation section)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ_pi3_eKVjw"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTkzkTvNkVkN"
      },
      "source": [
        "### Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2hJtOhiP6HSS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l3U8rnlkiwhR"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
        "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
        "                 epochs: int = 100,\n",
        "                 epoch: int = 0,\n",
        "                 notebook: bool = False\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion.cuda()\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.validation_DataLoader = validation_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.epoch = epoch\n",
        "        self.notebook = notebook\n",
        "\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.learning_rate = []\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        progressbar = trange(self.epochs, desc='Progress')\n",
        "        for i in progressbar:\n",
        "            \"\"\"Epoch counter\"\"\"\n",
        "            self.epoch += 1  # epoch counter\n",
        "\n",
        "            \"\"\"Training block\"\"\"\n",
        "            self._train()\n",
        "\n",
        "            \"\"\"Validation block\"\"\"\n",
        "            if self.validation_DataLoader is not None:\n",
        "                self._validate()\n",
        "\n",
        "            \"\"\"Learning rate scheduler block\"\"\"\n",
        "            if self.lr_scheduler is not None:\n",
        "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
        "                    self.lr_scheduler.step(\n",
        "                        self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
        "                else:\n",
        "                    self.lr_scheduler.step()  # learning rate scheduler step\n",
        "        return self.training_loss, self.validation_loss, self.learning_rate\n",
        "\n",
        "    def _train(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.train()  # train mode\n",
        "        train_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
        "                          leave=False)\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "            self.optimizer.zero_grad()  # zerograd the parameters\n",
        "            out = self.model(input)  # one forward pass\n",
        "            loss = self.criterion(out, target.long())  # calculate loss\n",
        "            loss_value = loss.item()\n",
        "            train_losses.append(loss_value)\n",
        "            loss.backward()  # one backward pass\n",
        "            self.optimizer.step()  # update the parameters\n",
        "\n",
        "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
        "\n",
        "        print(\"MEAN LOSS: {}\".format(np.mean(train_losses)))\n",
        "        self.training_loss.append(np.mean(train_losses))\n",
        "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        batch_iter.close()\n",
        "\n",
        "    def _validate(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.eval()  # evaluation mode\n",
        "        valid_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
        "                          leave=False)\n",
        "\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model(input)\n",
        "                loss = self.criterion(out, target.long())\n",
        "                loss_value = loss.item()\n",
        "                valid_losses.append(loss_value)\n",
        "\n",
        "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
        "\n",
        "        self.validation_loss.append(np.mean(valid_losses))\n",
        "\n",
        "        batch_iter.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6WaDZ6C4T-x"
      },
      "source": [
        "###  Data preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vDdgaaAY5J7"
      },
      "source": [
        "#### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NZXZ0QZd4eNi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample: tuple) -> tuple:\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img, mask = sample\n",
        "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "# Add your augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1WR3rYOZA64"
      },
      "source": [
        "#### Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hp2gICaf4W9q"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "class TrainDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for semantic segmentation. Data is stored as image list. \n",
        "    An image list file is a tsv file where each row contains the path to an image and its gt path.\n",
        "    \"\"\"\n",
        "    NUM_CLASSES = 19\n",
        "\n",
        "    def __init__(self, img_list_path: str, split=\"train\"):\n",
        "        \"\"\"\n",
        "        :param args: dataset args for training\n",
        "        :param img_list_path: path to the directory of image list files\n",
        "        :param split: type of dataset, train test or validation\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.files = {}\n",
        "        self.gts = {}\n",
        "        imgs = []\n",
        "        gts = []\n",
        "\n",
        "        # Parsing the image_list  \n",
        "        for dirpath, dirnames, filenames in os.walk(os.path.join(img_list_path, split)):\n",
        "            for file in filenames:\n",
        "                with open(os.path.join(dirpath, file), mode='r') as img_list:\n",
        "                    for row in img_list:\n",
        "                        _, img, gt = row.split('\\t')\n",
        "                        imgs.append(img)\n",
        "                        gts.append(gt.strip())\n",
        "\n",
        "        self.files[split] = imgs\n",
        "        self.gts[split] = gts\n",
        "\n",
        "        # Cityscapes classes \n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
        "        self.class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
        "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
        "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
        "                            'motorcycle', 'bicycle']\n",
        "\n",
        "        self.ignore_index = 255\n",
        "        \n",
        "        self.class_map = self.reduce_class_map()      \n",
        "        \n",
        "        self.set_augmentation_type()\n",
        "\n",
        "        if not self.files[split]:\n",
        "            raise Exception(\"No files for split=[%s] found in %s\" % (split, img_list_path))\n",
        "\n",
        "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[self.split][index].rstrip()\n",
        "        lbl_path = self.gts[self.split][index].rstrip()\n",
        "\n",
        "        _img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        _target = np.array(Image.open(lbl_path), dtype=np.uint8)\n",
        "\n",
        "        # # Encoding mask to contain only valid classes:\n",
        "        # self.encode_segmap(_target)\n",
        "        \n",
        "        _img = Image.fromarray(_img)\n",
        "        _target = Image.fromarray(_target)\n",
        "\n",
        "        sample = (_img, _target)\n",
        "        if self.split == 'train':\n",
        "            return self.transform_train(sample)\n",
        "        elif self.split == 'val':\n",
        "            return self.transform_validation(sample)\n",
        "        elif self.split == 'test':\n",
        "            return self.transform_test(sample)\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        # Put all void classes to zero\n",
        "        for _voidc in self.void_classes:\n",
        "            mask[mask == _voidc] = self.ignore_index\n",
        "        for _validc in self.valid_classes:\n",
        "            mask[mask == _validc] = self.class_map[_validc]\n",
        "        return mask\n",
        "\n",
        "    def transform_tr(self, sample):\n",
        "      # Add your training augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "        resize = transforms.Resize(size=(128, 128), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        img = resize(sample[0])\n",
        "        mask = resize(sample[1])\n",
        "        \n",
        "        sample = (img, mask)\n",
        "\n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_val(self, sample):\n",
        "      # Add your validation augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_ts(self, sample):\n",
        "      # Add your testing augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def get_img_list(self):\n",
        "        img_list = [(img_path, os.path.join(self.annotations_base,\n",
        "                                            img_path.split(os.sep)[-2],\n",
        "                                            os.path.basename(img_path)[:-15] + 'gtFine_labelIds.png')) for img_path in\n",
        "                    self.files[self.split]]\n",
        "        return img_list\n",
        "\n",
        "    def set_augmentation_type(self):\n",
        "        self.transform_train = self.transform_tr\n",
        "        self.transform_validation = self.transform_val\n",
        "        self.transform_test = self.transform_ts\n",
        "\n",
        "\n",
        "    def reduce_class_map(self):\n",
        "    # Reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”:\n",
        "    # The guidelines: \n",
        "    #   1. Everything that can be found on the road wil be marked as vehicle:\n",
        "    #       a. All vechile types.\n",
        "    #       b. Person and rider classes as well, as they can be found on the road, and should not be labeled as \"Road\"\n",
        "\n",
        "    # valid class id  | class name      | new class name    | new class id | \n",
        "    #     0           | unlabeled       |  unlabeld         |  0\n",
        "    #     7           | road            |  road             |  1\n",
        "    #     8           | sidewalk        |  background       |  2\n",
        "    #     11          | building        |  background       |  2\n",
        "    #     12          | wall            |  background       |  2\n",
        "    #     13          | fence           |  background       |  2\n",
        "    #     17          | pole            |  background       |  2\n",
        "    #     19          | traffic light   | background        |  2\n",
        "    #     20          | traffic sign    | background        |  2\n",
        "    #     21          | vegetation      | background        |  2\n",
        "    #     22          | terrain         |  background       |  2\n",
        "    #     23          | sky             |  background       |  2\n",
        "    #     24          | person          |  vehicle          |  3\n",
        "    #     25          | rider           |  vehicle          |  3\n",
        "    #     26          | car             |  vehicle          |  3\n",
        "    #     27          | truck           |  vehicle          |  3\n",
        "    #     28          | bus             |  vehicle          |  3\n",
        "    #     31          | train           |  vehicle          |  3\n",
        "    #     32          | motorcycle      |  vehicle          |  3\n",
        "    #     33          | bicycle         |  vehicle          |  3\n",
        "    \n",
        "        class_map = {0: 0, #unlabeled -> unlabeled\n",
        "                    7: 1, #road -> road\n",
        "                    8: 2, #sidewalk -> background\n",
        "                    11: 2, #building -> background\n",
        "                    12: 2, #wall -> background\n",
        "                    13: 2, #fence -> background\n",
        "                    17: 2, #pole -> background\n",
        "                    19: 2, #traffic light -> background\n",
        "                    20: 2, #traffic sign -> background\n",
        "                    21: 2, #vegetation -> background\n",
        "                    22: 2, #terrain -> background\n",
        "                    23: 2, #sky -> background\n",
        "                    24: 3, #person -> vehicle25\n",
        "                    25: 3, #rider -> vehicle\n",
        "                    26: 3, #car -> vehicle\n",
        "                    27: 3, #truck -> vehicle\n",
        "                    28: 3, #bus -> vehicle\n",
        "                    31: 3, #train -> vehicle\n",
        "                    32: 3, #motorcycle -> vehicle\n",
        "                    33: 3 #bicycle -> vehicle\n",
        "                    }\n",
        "        return class_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb5B-A7n4QoP"
      },
      "source": [
        "### Training Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts0HsEVuFdF8",
        "outputId": "f1430e2b-6f5a-494c-8321-0f7578e48226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unet in /opt/conda/lib/python3.8/site-packages (0.7.7)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from unet) (1.10.0a0+ecc3718)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->unet) (3.10.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cSB4ovx5FDRh"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.transform import resize\n",
        "from unet import UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6dThpmTxdoPV"
      },
      "outputs": [],
      "source": [
        "img_list_dir = \"/DATA/img_list\"\n",
        "# For example: img_list_dir = \"/content/drive/MyDrive/Algo_Home_Assignment/Cityscapes/img_list\"\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tHl1US3jLf",
        "outputId": "d7a6f47a-8f62-42a8-bc0b-2ae5f7bfa625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2975 train images\n",
            "Found 500 val images\n"
          ]
        }
      ],
      "source": [
        "train_set = TrainDataset(img_list_path=img_list_dir, split='train')\n",
        "val_set = TrainDataset(img_list_path=img_list_dir, split='val')\n",
        "dataloader_training = DataLoader(train_set, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_set, batch_size=1, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config():\n",
        "  def __init__(self) -> None:\n",
        "      self.EPOCHS = 10\n",
        "      # lr\n",
        "      self.lr = 0.01\n",
        "      self.momentum = 0.9\n",
        "      \n",
        "      # lr scheduler\n",
        "      self.multistep_milestones = [5, 8]\n",
        "      self.lr_gamma = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ipywidgets import IntProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u3LihqMx3nJo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7b2b54f8f33417284083be0f63af5fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Progress'), FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d52e3bac5522431c883ed8f8d62674c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)\n",
            "  return torch.floor_divide(self, other)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MEAN LOSS: nan\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b548ad2dd7c4b0f8fd30d4d7f854679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    print(\"CPU\")\n",
        "    torch.device('cpu')\n",
        "\n",
        "# model\n",
        "model = UNet(in_channels=3,\n",
        "             out_classes=19,\n",
        "             out_channels_first_layer=64,\n",
        "             num_encoding_blocks=2,\n",
        "             padding=2,\n",
        "             dimensions=2).to(device)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ---------------------------------  \"COMPLETE ME\"  ---------------------------\n",
        "config = Config()\n",
        "\n",
        "# Add a loss function \n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "\n",
        "## Add an optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
        "\n",
        "\n",
        "## Add a learning rate scheduler \n",
        "lr_scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=config.multistep_milestones, gamma=config.lr_gamma)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=dataloader_training,\n",
        "                  validation_DataLoader=dataloader_validation,\n",
        "                  lr_scheduler=lr_scheduler,\n",
        "                  epochs=100,\n",
        "                  epoch=0,\n",
        "                  notebook=True)\n",
        "\n",
        "# start training\n",
        "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWtze-dJj5Lc"
      },
      "source": [
        "GOOD LUCK !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VTkzkTvNkVkN",
        "-6WaDZ6C4T-x"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
