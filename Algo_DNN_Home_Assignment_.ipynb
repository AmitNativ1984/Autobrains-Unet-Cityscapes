{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VTkzkTvNkVkN",
        "-6WaDZ6C4T-x"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitNativ1984/Autobrains-Unet-Cityscapes/blob/main/Algo_DNN_Home_Assignment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 1 - Semantic Segmentation using U-net\n",
        "\n",
        "Identifying the drivable area provides critical information for navigating and path planning in autonomous driving. In this task you will take the first step torward this feature by segmenting the road in the image.\n",
        "\n",
        "\n",
        "# **Save Your Time**\n",
        "Please first download the data. The data is transfered via Google drive which somtimes slow.\n",
        "*   [Zip file - containing the data](https://drive.google.com/file/d/1_Demk2hTXuVPq9bBToedS4fOLqwaG7N3/view?usp=share_link)\n",
        "*   [Google drive with all the files](https://drive.google.com/drive/folders/10udfdPAT0yPq1TA7Ld0ddSKMedbEJBoe?usp=share_link)\n",
        "\n",
        "![](https://raw.githubusercontent.com/henyau/Image-Segmentation-with-Unet/master/images/train_label.png)\n",
        "\n",
        "\n",
        "You are provided with a partial training code of a U-net model. Your task is to train a model that predicts the road segment in the image. Please implement the missing components of the training pipeline, train the model and preform evaluation.\n",
        "\n",
        "### Data - road and vehicle segmentation\n",
        "\n",
        "The zip file provided contains images and high quality dense pixel annotations (fine annotation). The data is split into train (\\~2900 images) and validation (\\~500 images) with the directories similar the original Cityscapes dataset. \n",
        "\n",
        "The original Cityscapes anotations contains 19 classes. Please reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”.You can read more about Cityscape here https://www.cityscapes-dataset.com/dataset-overview/#labeling-policy.\n",
        "\n",
        "\n",
        "The Zip file provides with the following:\n",
        "1.   cityscapes dir - a directory with the images and annotations. Make sure the annotations contains the 3 classes mentioned above\n",
        "2.   image_list dir - a tsv file for train and validation. Each row in the tsv is a pairs of paths, path to image and path to annotation. In each TSV change **/PATH_TO_CITYSCAPES_DATA_DIR/** to the absolute path of cityscapes dir\n",
        "\n",
        "\n",
        "\n",
        "To save time:\n",
        "*   Resize images to a small fixed-size that will enable you to train the model in a fesiable time.\n",
        "\n",
        "# **Deliverables**\n",
        "\n",
        "\n",
        "1.   Complete the training code for road segmentation (marked \"COMPLETE ME\"):\n",
        "  * **Loss function** (Training Flow section)\n",
        "  * **Optimizer** (Training Flow section)\n",
        "  * **Augmentations** (Data preparations section)\n",
        "2.   **Perform an evaluation of your trained model** (Evaluation section)\n"
      ],
      "metadata": {
        "id": "GJm1RJJykGMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "GZ_pi3_eKVjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer Class"
      ],
      "metadata": {
        "id": "VTkzkTvNkVkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "2hJtOhiP6HSS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l3U8rnlkiwhR"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
        "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
        "                 epochs: int = 100,\n",
        "                 epoch: int = 0,\n",
        "                 notebook: bool = False\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion.cuda()\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.validation_DataLoader = validation_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.epoch = epoch\n",
        "        self.notebook = notebook\n",
        "\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.learning_rate = []\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        progressbar = trange(self.epochs, desc='Progress')\n",
        "        for i in progressbar:\n",
        "            \"\"\"Epoch counter\"\"\"\n",
        "            self.epoch += 1  # epoch counter\n",
        "\n",
        "            \"\"\"Training block\"\"\"\n",
        "            self._train()\n",
        "\n",
        "            \"\"\"Validation block\"\"\"\n",
        "            if self.validation_DataLoader is not None:\n",
        "                self._validate()\n",
        "\n",
        "            \"\"\"Learning rate scheduler block\"\"\"\n",
        "            if self.lr_scheduler is not None:\n",
        "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
        "                    self.lr_scheduler.batch(\n",
        "                        self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
        "                else:\n",
        "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
        "        return self.training_loss, self.validation_loss, self.learning_rate\n",
        "\n",
        "    def _train(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.train()  # train mode\n",
        "        train_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
        "                          leave=False)\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "            self.optimizer.zero_grad()  # zerograd the parameters\n",
        "            out = self.model(input)  # one forward pass\n",
        "            loss = self.criterion(out, target.long())  # calculate loss\n",
        "            loss_value = loss.item()\n",
        "            train_losses.append(loss_value)\n",
        "            loss.backward()  # one backward pass\n",
        "            self.optimizer.step()  # update the parameters\n",
        "\n",
        "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
        "\n",
        "        print(\"MEAN LOSS: {}\".format(np.mean(train_losses)))\n",
        "        self.training_loss.append(np.mean(train_losses))\n",
        "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        batch_iter.close()\n",
        "\n",
        "    def _validate(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.eval()  # evaluation mode\n",
        "        valid_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
        "                          leave=False)\n",
        "\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model(input)\n",
        "                loss = self.criterion(out, target)\n",
        "                loss_value = loss.item()\n",
        "                valid_losses.append(loss_value)\n",
        "\n",
        "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
        "\n",
        "        self.validation_loss.append(np.mean(valid_losses))\n",
        "\n",
        "        batch_iter.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Data preparations"
      ],
      "metadata": {
        "id": "-6WaDZ6C4T-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Augmentations"
      ],
      "metadata": {
        "id": "_vDdgaaAY5J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample: tuple) -> tuple:\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img, mask = sample\n",
        "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "\n",
        "        img = torch.from_numpy(img).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "# Add your augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "NZXZ0QZd4eNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset object"
      ],
      "metadata": {
        "id": "-1WR3rYOZA64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for semantic segmentation. Data is stored as image list. \n",
        "    An image list file is a tsv file where each row contains the path to an image and its gt path.\n",
        "    \"\"\"\n",
        "    NUM_CLASSES = 19\n",
        "\n",
        "    def __init__(self, img_list_path: str, split=\"train\"):\n",
        "        \"\"\"\n",
        "        :param args: dataset args for training\n",
        "        :param img_list_path: path to the directory of image list files\n",
        "        :param split: type of dataset, train test or validation\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.files = {}\n",
        "        self.gts = {}\n",
        "        imgs = []\n",
        "        gts = []\n",
        "\n",
        "        # Parsing the image_list  \n",
        "        for dirpath, dirnames, filenames in os.walk(os.path.join(img_list_path, split)):\n",
        "            for file in filenames:\n",
        "                with open(os.path.join(dirpath, file), mode='r') as img_list:\n",
        "                    for row in img_list:\n",
        "                        _, img, gt = row.split('\\t')\n",
        "                        imgs.append(img)\n",
        "                        gts.append(gt.strip())\n",
        "\n",
        "        self.files[split] = imgs\n",
        "        self.gts[split] = gts\n",
        "\n",
        "        # Cityscapes classes \n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
        "        self.class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
        "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
        "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
        "                            'motorcycle', 'bicycle']\n",
        "\n",
        "        self.ignore_index = 255\n",
        "        self.set_augmentation_type()\n",
        "\n",
        "        if not self.files[split]:\n",
        "            raise Exception(\"No files for split=[%s] found in %s\" % (split, img_list_path))\n",
        "\n",
        "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[self.split][index].rstrip()\n",
        "        lbl_path = self.gts[self.split][index].rstrip()\n",
        "\n",
        "        _img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        _target = np.array(Image.open(lbl_path), dtype=np.uint8)\n",
        "\n",
        "        _img = Image.fromarray(_img)\n",
        "        _target = Image.fromarray(_target)\n",
        "\n",
        "        sample = (_img, _target)\n",
        "        if self.split == 'train':\n",
        "            return self.transform_train(sample)\n",
        "        elif self.split == 'val':\n",
        "            return self.transform_validation(sample)\n",
        "        elif self.split == 'test':\n",
        "            return self.transform_test(sample)\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        # Put all void classes to zero\n",
        "        for _voidc in self.void_classes:\n",
        "            mask[mask == _voidc] = self.ignore_index\n",
        "        for _validc in self.valid_classes:\n",
        "            mask[mask == _validc] = self.class_map[_validc]\n",
        "        return mask\n",
        "\n",
        "    def transform_tr(self, sample):\n",
        "      # Add your training augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "       \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_val(self, sample):\n",
        "      # Add your validation augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_ts(self, sample):\n",
        "      # Add your testing augmentations here\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def get_img_list(self):\n",
        "        img_list = [(img_path, os.path.join(self.annotations_base,\n",
        "                                            img_path.split(os.sep)[-2],\n",
        "                                            os.path.basename(img_path)[:-15] + 'gtFine_labelIds.png')) for img_path in\n",
        "                    self.files[self.split]]\n",
        "        return img_list\n",
        "\n",
        "    def set_augmentation_type(self):\n",
        "        self.transform_train = self.transform_tr\n",
        "        self.transform_validation = self.transform_val\n",
        "        self.transform_test = self.transform_ts\n"
      ],
      "metadata": {
        "id": "hp2gICaf4W9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Flow"
      ],
      "metadata": {
        "id": "rb5B-A7n4QoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unet"
      ],
      "metadata": {
        "id": "Ts0HsEVuFdF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1430e2b-6f5a-494c-8321-0f7578e48226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unet\n",
            "  Downloading unet-0.7.7-py2.py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from unet) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->unet) (4.1.1)\n",
            "Installing collected packages: unet\n",
            "Successfully installed unet-0.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.transform import resize\n",
        "from unet import UNet"
      ],
      "metadata": {
        "id": "cSB4ovx5FDRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list_dir = \"PATH TO YOUR IMAGE_LIST DIR\"\n",
        "# For example: img_list_dir = \"/content/drive/MyDrive/Algo_Home_Assignment/Cityscapes/img_list\"\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "6dThpmTxdoPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = TrainDataset(img_list_path=img_list_dir, split='train')\n",
        "val_set = TrainDataset(img_list_path=img_list_dir, split='val')\n",
        "dataloader_training = DataLoader(train_set, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_set, batch_size=1, drop_last=True, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tHl1US3jLf",
        "outputId": "d7a6f47a-8f62-42a8-bc0b-2ae5f7bfa625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 train images\n",
            "Found 500 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    print(\"CPU\")\n",
        "    torch.device('cpu')\n",
        "\n",
        "# model\n",
        "model = UNet(in_channels=3,\n",
        "             out_classes=19,\n",
        "             out_channels_first_layer=64,\n",
        "             num_encoding_blocks=2,\n",
        "             padding=2,\n",
        "             dimensions=2).to(device)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ---------------------------------  \"COMPLETE ME\"  ---------------------------\n",
        "\n",
        "# Add a loss function \n",
        "criterion = None\n",
        "\n",
        "\n",
        "## Add an optimizer\n",
        "optimizer = None\n",
        "\n",
        "\n",
        "## Add a learning rate scheduler \n",
        "lr_scheduler=None,\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=dataloader_training,\n",
        "                  validation_DataLoader=dataloader_validation,\n",
        "                  lr_scheduler=None,\n",
        "                  epochs=100,\n",
        "                  epoch=0,\n",
        "                  notebook=True)\n",
        "\n",
        "# start training\n",
        "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
      ],
      "metadata": {
        "id": "u3LihqMx3nJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOD LUCK !"
      ],
      "metadata": {
        "id": "RWtze-dJj5Lc"
      }
    }
  ]
}