{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitNativ1984/Autobrains-Unet-Cityscapes/blob/main/Algo_DNN_Home_Assignment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJm1RJJykGMu"
      },
      "source": [
        "## TASK 1 - Semantic Segmentation using U-net\n",
        "\n",
        "Identifying the drivable area provides critical information for navigating and path planning in autonomous driving. In this task you will take the first step torward this feature by segmenting the road in the image.\n",
        "\n",
        "\n",
        "# **Save Your Time**\n",
        "Please first download the data. The data is transfered via Google drive which somtimes slow.\n",
        "*   [Zip file - containing the data](https://drive.google.com/file/d/1_Demk2hTXuVPq9bBToedS4fOLqwaG7N3/view?usp=share_link)\n",
        "*   [Google drive with all the files](https://drive.google.com/drive/folders/10udfdPAT0yPq1TA7Ld0ddSKMedbEJBoe?usp=share_link)\n",
        "\n",
        "![](https://raw.githubusercontent.com/henyau/Image-Segmentation-with-Unet/master/images/train_label.png)\n",
        "\n",
        "\n",
        "You are provided with a partial training code of a U-net model. Your task is to train a model that predicts the road segment in the image. Please implement the missing components of the training pipeline, train the model and preform evaluation.\n",
        "\n",
        "### Data - road and vehicle segmentation\n",
        "\n",
        "The zip file provided contains images and high quality dense pixel annotations (fine annotation). The data is split into train (\\~2900 images) and validation (\\~500 images) with the directories similar the original Cityscapes dataset. \n",
        "\n",
        "The original Cityscapes anotations contains 19 classes. Please reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”.You can read more about Cityscape here https://www.cityscapes-dataset.com/dataset-overview/#labeling-policy.\n",
        "\n",
        "\n",
        "The Zip file provides with the following:\n",
        "1.   cityscapes dir - a directory with the images and annotations. Make sure the annotations contains the 3 classes mentioned above\n",
        "2.   image_list dir - a tsv file for train and validation. Each row in the tsv is a pairs of paths, path to image and path to annotation. In each TSV change **/PATH_TO_CITYSCAPES_DATA_DIR/** to the absolute path of cityscapes dir\n",
        "\n",
        "\n",
        "\n",
        "To save time:\n",
        "*   Resize images to a small fixed-size that will enable you to train the model in a fesiable time.\n",
        "\n",
        "# **Deliverables**\n",
        "\n",
        "\n",
        "1.   Complete the training code for road segmentation (marked \"COMPLETE ME\"):\n",
        "  * **Loss function** (Training Flow section)\n",
        "  * **Optimizer** (Training Flow section)\n",
        "  * **Augmentations** (Data preparations section)\n",
        "2.   **Perform an evaluation of your trained model** (Evaluation section)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ_pi3_eKVjw"
      },
      "source": [
        "# **Thoughts and Notes**\n",
        "---\n",
        "1. Train simply and look at results for the first time\n",
        "2. Is the initial model trained? is it possible to do transfer learning / fine tuning?\n",
        "3. **USE TENSORBOARD**: Monitor the following:\n",
        "\n",
        "  3.1. Total Train loss\n",
        "\n",
        "  3.2.  Train loss per class\n",
        "  \n",
        "  3.3.  Validation loss\n",
        "\n",
        "  3.4.  Validation loss per class\n",
        "\n",
        "  3.5.  Validation images\n",
        "\n",
        "*   weight differently unimportant classes\n",
        "*   focal loss\n",
        "*   crop original size into diffferent scales\n",
        "* Penalize the road more!\n",
        "\n",
        "## Questions:\n",
        "* Should evel part be provided?\n",
        "\n",
        "## Augmentations:\n",
        "1. normalized cityscapes RGB values\n",
        "2. resolusion / scale - keep aspect ratio\n",
        "3. albumentations: weather condition\n",
        "4. flips (left/right)\n",
        "5. blurs - gaussian/motion\n",
        "\n",
        "6. Pay attension to augmentations for valdiation and test.\n",
        "7. Pay attension to downsampling and required augmenations for masks!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTkzkTvNkVkN"
      },
      "source": [
        "### Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2hJtOhiP6HSS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "l3U8rnlkiwhR"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
        "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
        "                 epochs: int = 100,\n",
        "                 epoch: int = 0,\n",
        "                 notebook: bool = False\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion.cuda()\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.validation_DataLoader = validation_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.epoch = epoch\n",
        "        self.notebook = notebook\n",
        "\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.learning_rate = []\n",
        "\n",
        "        self.tb_writer = SummaryWriter()\n",
        "\n",
        "        # create dir for saving model\n",
        "        if not os.path.exists(\"models\"):\n",
        "            os.mkdir(\"models\")\n",
        "\n",
        "        self.model_path = \"models/\"\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        progressbar = trange(self.epochs, desc='Progress')\n",
        "        for i in progressbar:\n",
        "            \"\"\"Epoch counter\"\"\"\n",
        "            self.epoch += 1  # epoch counter\n",
        "\n",
        "            \"\"\"Training block\"\"\"\n",
        "            self._train()\n",
        "\n",
        "            \"\"\"Validation block\"\"\"\n",
        "            if self.validation_DataLoader is not None:\n",
        "                self._validate()\n",
        "\n",
        "            \"\"\"Learning rate scheduler block\"\"\"\n",
        "            if self.lr_scheduler is not None:\n",
        "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
        "                    self.lr_scheduler.step(\n",
        "                        self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
        "                else:\n",
        "                    self.lr_scheduler.step()  # learning rate scheduler step\n",
        "        return self.training_loss, self.validation_loss, self.learning_rate\n",
        "\n",
        "    def _train(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.train()  # train mode\n",
        "        train_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
        "                          leave=False)\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "            self.optimizer.zero_grad()  # zerograd the parameters\n",
        "            out = self.model(input)  # one forward pass\n",
        "            loss = self.criterion(out, target.long())  # calculate loss\n",
        "            loss_value = loss.item()\n",
        "            train_losses.append(loss_value)\n",
        "            loss.backward()  # one backward pass\n",
        "            self.optimizer.step()  # update the parameters\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                self.write_predictions_to_tensorboard(out, 'train_predictions')\n",
        "\n",
        "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
        "        print(\"TRAIN MEAN LOSS: {}\".format(np.mean(train_losses)))\n",
        "        self.training_loss.append(np.mean(train_losses))\n",
        "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        self.tb_writer.add_scalar('Loss/train', np.mean(train_losses), self.epoch)\n",
        "        self.tb_writer.add_scalar('Learning rate', self.optimizer.param_groups[0]['lr'], self.epoch)\n",
        "        \n",
        "        # saving current model:\n",
        "        torch.save(self.model.state_dict(), self.model_path + \"last.pth\".format(self.epoch))\n",
        "\n",
        "        # save checkpoint every 5 epochs\n",
        "        if self.epoch % 5 == 0:\n",
        "            torch.save(self.model.state_dict(), self.model_path + \"checkpoint_{}.pth\".format(self.epoch))\n",
        "\n",
        "        batch_iter.close()\n",
        "\n",
        "    def _validate(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.eval()  # evaluation mode\n",
        "        valid_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
        "                          leave=False)\n",
        "\n",
        "        dice_scores = []\n",
        "\n",
        "        img2save = random.randint(0, len(self.validation_DataLoader))\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model(input)\n",
        "                loss = self.criterion(out, target.long())\n",
        "                loss_value = loss.item()\n",
        "                valid_losses.append(loss_value)\n",
        "\n",
        "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
        "        \n",
        "            # add images to tensorboard every 10 batches\n",
        "            if i == img2save:\n",
        "               self.write_predictions_to_tensorboard(out, 'val_predictions')\n",
        "\n",
        "            # evaluate dice score\n",
        "            # convert to one-hot format\n",
        "            target_19classes = target.clone()\n",
        "            target_19classes[target == 255] = 18\n",
        "            mask_onehot = F.one_hot(target_19classes.long(), 19).permute(0, 3, 1, 2).float()\n",
        "            pred_onehot = F.one_hot(out.argmax(dim=1), 19).permute(0, 3, 1, 2).float()\n",
        "            # compute the Dice score, averaged over only the first 3 classes\n",
        "            dice_batch = self.multiclass_dice_coeff(pred_onehot[:, :3], mask_onehot[:, :3], reduce_batch_first=False)\n",
        "            dice_scores.append(dice_batch.cpu().numpy())\n",
        "\n",
        "        self.validation_loss.append(np.mean(valid_losses))\n",
        "        self.tb_writer.add_scalar('Loss/val', np.mean(valid_losses), self.epoch)\n",
        "        print(\"VAL MEAN LOSS: {}\".format(np.mean(valid_losses)))\n",
        "\n",
        "        self.tb_writer.add_scalar('Dice/val', np.mean(dice_scores), self.epoch)\n",
        "        print(\"VAL MEAN DICE SCORE: {}\".format(np.mean(dice_scores)))\n",
        "\n",
        "        batch_iter.close()\n",
        "\n",
        "    def decode_pred(self, pred):\n",
        "        # Put all void classes to zero\n",
        "        \n",
        "        labeled_pred = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "        labeled_pred[pred == 0,:] = [0, 0, 255]    #road\n",
        "        labeled_pred[pred == 1,:] = [255, 255, 0]  # background\n",
        "        labeled_pred[pred == 2,:] = [0, 255, 0]  # vehicle\n",
        "        \n",
        "        return labeled_pred\n",
        "\n",
        "    def write_predictions_to_tensorboard(self, out, tb_label):\n",
        "         # convert out to labels in color\n",
        "            pred = torch.softmax(out, dim=1)\n",
        "            pred = torch.argmax(out, dim=1)\n",
        "            pred = pred.cpu().detach().numpy()\n",
        "            pred = np.array([self.decode_pred(x) for x in pred])\n",
        "            \n",
        "            # pred = torch.from_numpy(pred).float()\n",
        "            \n",
        "            # convert pred to tensor\n",
        "            pred = torch.from_numpy(pred).float()\n",
        "            \n",
        "            # concatenate images to one image\n",
        "            pred = torch.cat([pred[i] for i in range(pred.shape[0])], dim=1)\n",
        "\n",
        "            \n",
        "            # pred = pred.view(pred.shape[1], -1, 3)\n",
        "            \n",
        "            self.tb_writer.add_image(tb_label, pred.numpy().astype(np.uint8), global_step=self.epoch, dataformats='HWC')\n",
        "            self.tb_writer.flush()\n",
        "            \n",
        "        \n",
        "\n",
        "    def dice_coeff(self, input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "        # Average of Dice coefficient for all batches, or for a single mask\n",
        "        assert input.size() == target.size()\n",
        "        assert input.dim() == 3 or not reduce_batch_first\n",
        "\n",
        "        sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
        "\n",
        "        inter = 2 * (input * target).sum(dim=sum_dim)\n",
        "        sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
        "        sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
        "\n",
        "        dice = (inter + epsilon) / (sets_sum + epsilon)\n",
        "        return dice.mean()\n",
        "\n",
        "\n",
        "    def multiclass_dice_coeff(self, input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "        # Average of Dice coefficient for all classes\n",
        "        return self.dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6WaDZ6C4T-x"
      },
      "source": [
        "###  Data preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vDdgaaAY5J7"
      },
      "source": [
        "#### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NZXZ0QZd4eNi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "from torchvision import transforms\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample: tuple) -> tuple:\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img, mask = sample\n",
        "        \n",
        "        img, mask = self.resize(img, mask, (256, 256))\n",
        "\n",
        "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "                \n",
        "        img = torch.from_numpy(img).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        \n",
        "        return img, mask\n",
        "\n",
        "# Add your augmentations here\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "    def normalize(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        # Normalize the image\n",
        "        normalize = [\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        img = transforms.Compose(normalize)(img)\n",
        "        return img\n",
        "\n",
        "    def resize(self, img, mask, size: tuple):\n",
        "        \n",
        "        # Resize\n",
        "        resize_img = transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
        "        resize_mask = transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST)\n",
        "                \n",
        "        # Resize the image\n",
        "        img = transforms.Compose([resize_img])(img)\n",
        "        mask = transforms.Compose([resize_mask])(mask)\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1WR3rYOZA64"
      },
      "source": [
        "#### Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "hp2gICaf4W9q"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "class TrainDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for semantic segmentation. Data is stored as image list. \n",
        "    An image list file is a tsv file where each row contains the path to an image and its gt path.\n",
        "    \"\"\"\n",
        "    NUM_CLASSES = 19\n",
        "\n",
        "    def __init__(self, img_list_path: str, split=\"train\"):\n",
        "        \"\"\"\n",
        "        :param args: dataset args for training\n",
        "        :param img_list_path: path to the directory of image list files\n",
        "        :param split: type of dataset, train test or validation\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.files = {}\n",
        "        self.gts = {}\n",
        "        imgs = []\n",
        "        gts = []\n",
        "\n",
        "        # Parsing the image_list  \n",
        "        for dirpath, dirnames, filenames in os.walk(os.path.join(img_list_path, split)):\n",
        "            for file in filenames:\n",
        "                with open(os.path.join(dirpath, file), mode='r') as img_list:\n",
        "                    for row in img_list:\n",
        "                        _, img, gt = row.split('\\t')\n",
        "                        imgs.append(img)\n",
        "                        gts.append(gt.strip())\n",
        "\n",
        "        self.files[split] = imgs\n",
        "        self.gts[split] = gts\n",
        "\n",
        "        # Cityscapes classes \n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
        "        self.class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
        "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
        "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
        "                            'motorcycle', 'bicycle']\n",
        "\n",
        "        self.ignore_index = 255\n",
        "        \n",
        "        self.class_map = self.reduce_class_map()      \n",
        "        \n",
        "        self.set_augmentation_type()\n",
        "\n",
        "        if not self.files[split]:\n",
        "            raise Exception(\"No files for split=[%s] found in %s\" % (split, img_list_path))\n",
        "\n",
        "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[self.split][index].rstrip()\n",
        "        lbl_path = self.gts[self.split][index].rstrip()\n",
        "\n",
        "        _img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        _target = np.array(Image.open(lbl_path), dtype=np.uint8)\n",
        "\n",
        "        # # Encoding mask to contain only valid classes:\n",
        "        # self.encode_segmap(_target)\n",
        "        \n",
        "        _img = Image.fromarray(_img)\n",
        "        _target = Image.fromarray(_target)\n",
        "\n",
        "        sample = (_img, _target)\n",
        "        if self.split == 'train':\n",
        "            return self.transform_train(sample)\n",
        "        elif self.split == 'val':\n",
        "            return self.transform_validation(sample)\n",
        "        elif self.split == 'test':\n",
        "            return self.transform_test(sample)\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        # Put all void classes to zero\n",
        "        for _voidc in self.void_classes:\n",
        "            mask[mask == _voidc] = self.ignore_index\n",
        "        for _validc in self.valid_classes:\n",
        "            mask[mask == _validc] = self.class_map[_validc]\n",
        "        return mask\n",
        "\n",
        "    def set_seed(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "       \n",
        "    \n",
        "    def transform_tr(self, sample):                              \n",
        "        # geometric_transforms = [\n",
        "        #     transforms.RandomApply(torch.nn.ModuleList([\n",
        "        #                                 transforms.RandomCrop((512, 1024)),\n",
        "        #                                 ]), \n",
        "        #                             p=0.2),\n",
        "        #     transforms.RandomHorizontalFlip(p=0.5),\n",
        "        # ]\n",
        "\n",
        "        # color_transforms = [\n",
        "        #     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
        "        # ]\n",
        "        \n",
        "        # state = torch.get_rng_state()\n",
        "        # img = sample[0]\n",
        "        # img_transforms = transforms.Compose(geometric_transforms + color_transforms)\n",
        "        # img_aug = img_transforms(img)\n",
        "\n",
        "        # torch.set_rng_state(state)\n",
        "        # mask = sample[1]\n",
        "        # mask_transforms = transforms.Compose(geometric_transforms)\n",
        "        # mask_aug = mask_transforms(mask)\n",
        "\n",
        "        # sample = (img_aug, mask_aug)\n",
        "\n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_val(self, sample):\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_ts(self, sample):\n",
        "\n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def get_img_list(self):\n",
        "        img_list = [(img_path, os.path.join(self.annotations_base,\n",
        "                                            img_path.split(os.sep)[-2],\n",
        "                                            os.path.basename(img_path)[:-15] + 'gtFine_labelIds.png')) for img_path in\n",
        "                    self.files[self.split]]\n",
        "        return img_list\n",
        "\n",
        "    def set_augmentation_type(self):\n",
        "        self.transform_train = self.transform_tr\n",
        "        self.transform_validation = self.transform_val\n",
        "        self.transform_test = self.transform_ts\n",
        "\n",
        "    def reduce_class_map(self):\n",
        "    # Reduce the number of classes from 19 to the followi[0,...]   |  2\n",
        "    #     21          | vegetation      | background        |  2\n",
        "    #     22          | terrain         |  background       |  2\n",
        "    #     23          | sky             |  background       |  2\n",
        "    #     24          | person          |  vehicle          |  3\n",
        "    #     25          | rider           |  vehicle          |  3\n",
        "    #     26          | car             |  vehicle          |  3\n",
        "    #     27          | truck           |  vehicle          |  3\n",
        "    #     28          | bus             |  vehicle          |  3\n",
        "    #     31          | train           |  vehicle          |  3\n",
        "    #     32          | motorcycle      |  vehicle          |  3\n",
        "    #     33          | bicycle         |  vehicle          |  3\n",
        "    \n",
        "        class_map = {0: 0, #unlabeled -> unlabeled\n",
        "                    7: 1, #road -> road\n",
        "                    8: 2, #sidewalk -> background\n",
        "                    11: 2, #building -> background\n",
        "                    12: 2, #wall -> background\n",
        "                    13: 2, #fence -> background\n",
        "                    17: 2, #pole -> background\n",
        "                    19: 2, #traffic light -> background\n",
        "                    20: 2, #traffic sign -> background\n",
        "                    21: 2, #vegetation -> background\n",
        "                    22: 2, #terrain -> background\n",
        "                    23: 2, #sky -> background\n",
        "                    24: 3, #person -> vehicle25\n",
        "                    25: 3, #rider -> vehicle\n",
        "                    26: 3, #car -> vehicle\n",
        "                    27: 3, #truck -> vehicle\n",
        "                    28: 3, #bus -> vehicle\n",
        "                    31: 3, #train -> vehicle\n",
        "                    32: 3, #motorcycle -> vehicle\n",
        "                    33: 3 #bicycle -> vehicle\n",
        "                    }\n",
        "\n",
        "        return class_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb5B-A7n4QoP"
      },
      "source": [
        "### Training Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts0HsEVuFdF8",
        "outputId": "f1430e2b-6f5a-494c-8321-0f7578e48226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unet in /opt/conda/lib/python3.8/site-packages (0.7.7)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from unet) (1.10.0a0+ecc3718)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->unet) (3.10.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cSB4ovx5FDRh"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.transform import resize\n",
        "from unet import UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6dThpmTxdoPV"
      },
      "outputs": [],
      "source": [
        "img_list_dir = \"/DATA/img_list\"\n",
        "# For example: img_list_dir = \"/content/drive/MyDrive/Algo_Home_Assignment/Cityscapes/img_list\"\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tHl1US3jLf",
        "outputId": "d7a6f47a-8f62-42a8-bc0b-2ae5f7bfa625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2975 train images\n",
            "Found 500 val images\n"
          ]
        }
      ],
      "source": [
        "train_set = TrainDataset(img_list_path=img_list_dir, split='train')\n",
        "val_set = TrainDataset(img_list_path=img_list_dir, split='val')\n",
        "dataloader_training = DataLoader(train_set, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_set, batch_size=1, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config():\n",
        "  def __init__(self) -> None:\n",
        "      self.EPOCHS = 25\n",
        "      # lr\n",
        "      self.lr = 0.001 # so far the best was 0.0001\n",
        "      self.momentum = 0.9\n",
        "      \n",
        "      # lr scheduler\n",
        "      self.multistep_milestones = [10, 20]\n",
        "      self.lr_gamma = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "u3LihqMx3nJo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e74af48d6c44cd95b1b6b38c4638f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Progress'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a300340eefbf408b97c10c73c8a180a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN MEAN LOSS: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36d955d779674320bae1748992474935",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL MEAN LOSS: 7.716314471006394\n",
            "VAL MEAN DICE SCORE: 0.2020431011915207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "455cd142c1a9447ebcdb85acf3a4e74a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN MEAN LOSS: nan\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33bd19e2c0c747429e98b8400b56490a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL MEAN LOSS: 7.716314471006394\n",
            "VAL MEAN DICE SCORE: 0.2020431011915207\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9185c23de8084661b0b928f512cb8c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN MEAN LOSS: nan\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b666366c843646d3ae29560b89c8d30d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL MEAN LOSS: 7.716314471006394\n",
            "VAL MEAN DICE SCORE: 0.2020431011915207\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e578b1798bdf42e4bc1880b7c9699894",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN MEAN LOSS: nan\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbe224fc4bef42869dcaede096ef9301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3132/2320266793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3132/3251700659.py\u001b[0m in \u001b[0;36mrun_trainer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"\"\"Validation block\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_DataLoader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m\"\"\"Learning rate scheduler block\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3132/3251700659.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mimg2save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_DataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# send to device (GPU or CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3132/363286090.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlbl_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0m_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0m_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \"\"\"\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    print(\"CPU\")\n",
        "    torch.device('cpu')\n",
        "\n",
        "# model\n",
        "model = UNet(in_channels=3,\n",
        "             out_classes=19,\n",
        "             out_channels_first_layer=64,\n",
        "             num_encoding_blocks=2,\n",
        "             padding=2,\n",
        "             dimensions=2).to(device)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ---------------------------------  \"COMPLETE ME\"  ---------------------------\n",
        "config = Config()\n",
        "\n",
        "# Add a loss function \n",
        "org_cityscapes_class_weights = [0.8373, 0.9180, 0.8660, 1.0345, 1.0166, 0.9969, 0.9754,\n",
        "                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037,\n",
        "                        1.0865, 1.0955, 1.0865, 1.1529, 1.0507]\n",
        "\n",
        "class_remapping = {\"0\": 0, \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1, \"6\": 1, \"7\": 1, \"8\": 1, \"9\": 1, \"10\": 1, \"11\": 1, \"12\": 1, \"13\": 2, \"14\": 2, \"15\": 2, \"16\": 2, \"17\": 2, \"18\": 2, \"255\": 255}\n",
        "class_weights = np.zeros(19)\n",
        "for cls_id in class_remapping.keys():\n",
        "    if class_remapping[cls_id] != 255:\n",
        "        class_weights[class_remapping[cls_id]] += org_cityscapes_class_weights[int(cls_id)]\n",
        "\n",
        "#weight more for less frequent classes\n",
        "class_weights[:3] = np.array([1, 0.1, 0.5]) #1/class_weights[:3]\n",
        "class_weights[3:] = 1\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(device), ignore_index=255)\n",
        "# criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "\n",
        "## Add an optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
        "\n",
        "\n",
        "## Add a learning rate scheduler \n",
        "lr_scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=config.multistep_milestones, gamma=config.lr_gamma)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=dataloader_training,\n",
        "                  validation_DataLoader=dataloader_validation,\n",
        "                  lr_scheduler=lr_scheduler,\n",
        "                  epochs=config.EPOCHS,\n",
        "                  epoch=0,\n",
        "                  notebook=True)\n",
        "\n",
        "# start training\n",
        "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWtze-dJj5Lc"
      },
      "source": [
        "GOOD LUCK !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VTkzkTvNkVkN",
        "-6WaDZ6C4T-x"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
