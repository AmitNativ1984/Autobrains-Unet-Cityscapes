{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitNativ1984/Autobrains-Unet-Cityscapes/blob/main/Algo_DNN_Home_Assignment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJm1RJJykGMu"
      },
      "source": [
        "## TASK 1 - Semantic Segmentation using U-net\n",
        "\n",
        "Identifying the drivable area provides critical information for navigating and path planning in autonomous driving. In this task you will take the first step torward this feature by segmenting the road in the image.\n",
        "\n",
        "\n",
        "# **Save Your Time**\n",
        "Please first download the data. The data is transfered via Google drive which somtimes slow.\n",
        "*   [Zip file - containing the data](https://drive.google.com/file/d/1_Demk2hTXuVPq9bBToedS4fOLqwaG7N3/view?usp=share_link)\n",
        "*   [Google drive with all the files](https://drive.google.com/drive/folders/10udfdPAT0yPq1TA7Ld0ddSKMedbEJBoe?usp=share_link)\n",
        "\n",
        "![](https://raw.githubusercontent.com/henyau/Image-Segmentation-with-Unet/master/images/train_label.png)\n",
        "\n",
        "\n",
        "You are provided with a partial training code of a U-net model. Your task is to train a model that predicts the road segment in the image. Please implement the missing components of the training pipeline, train the model and preform evaluation.\n",
        "\n",
        "### Data - road and vehicle segmentation\n",
        "\n",
        "The zip file provided contains images and high quality dense pixel annotations (fine annotation). The data is split into train (\\~2900 images) and validation (\\~500 images) with the directories similar the original Cityscapes dataset. \n",
        "\n",
        "The original Cityscapes anotations contains 19 classes. Please reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”.You can read more about Cityscape here https://www.cityscapes-dataset.com/dataset-overview/#labeling-policy.\n",
        "\n",
        "\n",
        "The Zip file provides with the following:\n",
        "1.   cityscapes dir - a directory with the images and annotations. Make sure the annotations contains the 3 classes mentioned above\n",
        "2.   image_list dir - a tsv file for train and validation. Each row in the tsv is a pairs of paths, path to image and path to annotation. In each TSV change **/PATH_TO_CITYSCAPES_DATA_DIR/** to the absolute path of cityscapes dir\n",
        "\n",
        "\n",
        "\n",
        "To save time:\n",
        "*   Resize images to a small fixed-size that will enable you to train the model in a fesiable time.\n",
        "\n",
        "# **Deliverables**\n",
        "\n",
        "\n",
        "1.   Complete the training code for road segmentation (marked \"COMPLETE ME\"):\n",
        "  * **Loss function** (Training Flow section)\n",
        "  * **Optimizer** (Training Flow section)\n",
        "  * **Augmentations** (Data preparations section)\n",
        "2.   **Perform an evaluation of your trained model** (Evaluation section)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ_pi3_eKVjw"
      },
      "source": [
        "# **Thoughts and Notes**\n",
        "---\n",
        "1. Train simply and look at results for the first time\n",
        "2. Is the initial model trained? is it possible to do transfer learning / fine tuning?\n",
        "3. **USE TENSORBOARD**: Monitor the following:\n",
        "\n",
        "  3.1. Total Train loss\n",
        "\n",
        "  3.2.  Train loss per class\n",
        "  \n",
        "  3.3.  Validation loss\n",
        "\n",
        "  3.4.  Validation loss per class\n",
        "\n",
        "  3.5.  Validation images\n",
        "\n",
        "*   weight differently unimportant classes\n",
        "*   focal loss\n",
        "*   crop original size into diffferent scales\n",
        "* Penalize the road more!\n",
        "\n",
        "## Questions:\n",
        "* Should evel part be provided?\n",
        "\n",
        "## Augmentations:\n",
        "1. normalized cityscapes RGB values\n",
        "2. resolusion / scale - keep aspect ratio\n",
        "3. albumentations: weather condition\n",
        "4. flips (left/right)\n",
        "5. blurs - gaussian/motion\n",
        "\n",
        "6. Pay attension to augmentations for valdiation and test.\n",
        "7. Pay attension to downsampling and required augmenations for masks!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTkzkTvNkVkN"
      },
      "source": [
        "### Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2hJtOhiP6HSS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "l3U8rnlkiwhR"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
        "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
        "                 epochs: int = 100,\n",
        "                 epoch: int = 0,\n",
        "                 notebook: bool = False\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion.cuda()\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.validation_DataLoader = validation_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.epoch = epoch\n",
        "        self.notebook = notebook\n",
        "\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.learning_rate = []\n",
        "\n",
        "        self.tb_writer = SummaryWriter()\n",
        "\n",
        "        # create dir for saving model\n",
        "        if not os.path.exists(\"models\"):\n",
        "            os.mkdir(\"models\")\n",
        "\n",
        "        self.model_path = \"models/\"\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        progressbar = trange(self.epochs, desc='Progress')\n",
        "        for i in progressbar:\n",
        "            \"\"\"Epoch counter\"\"\"\n",
        "            self.epoch += 1  # epoch counter\n",
        "\n",
        "            \"\"\"Training block\"\"\"\n",
        "            self._train()\n",
        "\n",
        "            \"\"\"Validation block\"\"\"\n",
        "            if self.validation_DataLoader is not None:\n",
        "                self._validate()\n",
        "\n",
        "            \"\"\"Learning rate scheduler block\"\"\"\n",
        "            if self.lr_scheduler is not None:\n",
        "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
        "                    self.lr_scheduler.step(\n",
        "                        self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
        "                else:\n",
        "                    self.lr_scheduler.step()  # learning rate scheduler step\n",
        "        return self.training_loss, self.validation_loss, self.learning_rate\n",
        "\n",
        "    def _train(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.train()  # train mode\n",
        "        train_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
        "                          leave=False)\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "            self.optimizer.zero_grad()  # zerograd the parameters\n",
        "            out = self.model(input)  # one forward pass\n",
        "            loss = self.criterion(out, target.long())  # calculate loss\n",
        "            loss_value = loss.item()\n",
        "            train_losses.append(loss_value)\n",
        "            loss.backward()  # one backward pass\n",
        "            self.optimizer.step()  # update the parameters\n",
        "\n",
        "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
        "            \n",
        "        print(\"MEAN LOSS: {}\".format(np.mean(train_losses)))\n",
        "        self.training_loss.append(np.mean(train_losses))\n",
        "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        self.tb_writer.add_scalar('Loss/train', np.mean(train_losses), self.epoch)\n",
        "        self.tb_writer.add_scalar('Learning rate', self.optimizer.param_groups[0]['lr'], self.epoch)\n",
        "        \n",
        "        # saving current model:\n",
        "        torch.save(self.model.state_dict(), self.model_path + \"last.pth\".format(self.epoch))\n",
        "\n",
        "        # save checkpoint every 5 epochs\n",
        "        if self.epoch % 5 == 0:\n",
        "            torch.save(self.model.state_dict(), self.model_path + \"checkpoint_{}.pth\".format(self.epoch))\n",
        "\n",
        "        batch_iter.close()\n",
        "\n",
        "    def _validate(self):\n",
        "\n",
        "        if self.notebook:\n",
        "            from tqdm.notebook import tqdm, trange\n",
        "        else:\n",
        "            from tqdm import tqdm, trange\n",
        "\n",
        "        self.model.eval()  # evaluation mode\n",
        "        valid_losses = []  # accumulate the losses here\n",
        "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
        "                          leave=False)\n",
        "\n",
        "        for i, (x, y) in batch_iter:\n",
        "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.model(input)\n",
        "                loss = self.criterion(out, target.long())\n",
        "                loss_value = loss.item()\n",
        "                valid_losses.append(loss_value)\n",
        "\n",
        "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
        "        \n",
        "            # add images to tensorboard every 10 batches\n",
        "            if i % 10 == 0:\n",
        "                # convert out to labels in color\n",
        "                out = torch.argmax(out, dim=1)\n",
        "                out = out.cpu().detach().numpy()\n",
        "                out = np.expand_dims(out, axis=1)\n",
        "                out = np.repeat(out, 3, axis=1)\n",
        "                out = np.transpose(out, (0, 2, 3, 1))\n",
        "                out = np.array([self.training_DataLoader.dataset.decode_segmap(x) for x in out])\n",
        "                out = np.transpose(out, (0, 3, 1, 2))\n",
        "                out = torch.from_numpy(out).float()\n",
        "                \n",
        "\n",
        "                \n",
        "                self.tb_writer.add_images('val_batch', target, self.epoch)\n",
        "\n",
        "        self.validation_loss.append(np.mean(valid_losses))\n",
        "        print(\"VAL MEAN LOSS: {}\".format(np.mean(valid_losses)))\n",
        "\n",
        "        self.tb_writer.add_scalar('Loss/val', np.mean(valid_losses), self.epoch)\n",
        "\n",
        "\n",
        "        batch_iter.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6WaDZ6C4T-x"
      },
      "source": [
        "###  Data preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vDdgaaAY5J7"
      },
      "source": [
        "#### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NZXZ0QZd4eNi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "from torchvision import transforms\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample: tuple) -> tuple:\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img, mask = sample\n",
        "        \n",
        "        img, mask = self.resize(img, mask, (256, 256))\n",
        "\n",
        "        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "                \n",
        "        img = torch.from_numpy(img).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        # img = self.normalize(img)\n",
        "        \n",
        "        return img, mask\n",
        "\n",
        "# Add your augmentations here\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# ----------------------------\"COMPLETE ME\"------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "    def normalize(self, img: torch.Tensor) -> torch.Tensor:\n",
        "        # Normalize the image\n",
        "        normalize = [\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        img = transforms.Compose(normalize)(img)\n",
        "        return img\n",
        "\n",
        "    def resize(self, img, mask, size: tuple):\n",
        "        \n",
        "        # Resize\n",
        "        resize_img = transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC)\n",
        "        resize_mask = transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST)\n",
        "                \n",
        "        # Resize the image\n",
        "        img = transforms.Compose([resize_img])(img)\n",
        "        mask = transforms.Compose([resize_mask])(mask)\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1WR3rYOZA64"
      },
      "source": [
        "#### Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hp2gICaf4W9q"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "class TrainDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for semantic segmentation. Data is stored as image list. \n",
        "    An image list file is a tsv file where each row contains the path to an image and its gt path.\n",
        "    \"\"\"\n",
        "    NUM_CLASSES = 19\n",
        "\n",
        "    def __init__(self, img_list_path: str, split=\"train\"):\n",
        "        \"\"\"\n",
        "        :param args: dataset args for training\n",
        "        :param img_list_path: path to the directory of image list files\n",
        "        :param split: type of dataset, train test or validation\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.files = {}\n",
        "        self.gts = {}\n",
        "        imgs = []\n",
        "        gts = []\n",
        "\n",
        "        # Parsing the image_list  \n",
        "        for dirpath, dirnames, filenames in os.walk(os.path.join(img_list_path, split)):\n",
        "            for file in filenames:\n",
        "                with open(os.path.join(dirpath, file), mode='r') as img_list:\n",
        "                    for row in img_list:\n",
        "                        _, img, gt = row.split('\\t')\n",
        "                        imgs.append(img)\n",
        "                        gts.append(gt.strip())\n",
        "\n",
        "        self.files[split] = imgs\n",
        "        self.gts[split] = gts\n",
        "\n",
        "        # Cityscapes classes \n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
        "        self.class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
        "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
        "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
        "                            'motorcycle', 'bicycle']\n",
        "\n",
        "        self.ignore_index = 255\n",
        "        \n",
        "        self.class_map = self.reduce_class_map()      \n",
        "        \n",
        "        self.set_augmentation_type()\n",
        "\n",
        "        if not self.files[split]:\n",
        "            raise Exception(\"No files for split=[%s] found in %s\" % (split, img_list_path))\n",
        "\n",
        "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[self.split][index].rstrip()\n",
        "        lbl_path = self.gts[self.split][index].rstrip()\n",
        "\n",
        "        _img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        _target = np.array(Image.open(lbl_path), dtype=np.uint8)\n",
        "\n",
        "        # # Encoding mask to contain only valid classes:\n",
        "        # self.encode_segmap(_target)\n",
        "        \n",
        "        _img = Image.fromarray(_img)\n",
        "        _target = Image.fromarray(_target)\n",
        "\n",
        "        sample = (_img, _target)\n",
        "        if self.split == 'train':\n",
        "            return self.transform_train(sample)\n",
        "        elif self.split == 'val':\n",
        "            return self.transform_validation(sample)\n",
        "        elif self.split == 'test':\n",
        "            return self.transform_test(sample)\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        # Put all void classes to zero\n",
        "        for _voidc in self.void_classes:\n",
        "            mask[mask == _voidc] = self.ignore_index\n",
        "        for _validc in self.valid_classes:\n",
        "            mask[mask == _validc] = self.class_map[_validc]\n",
        "        return mask\n",
        "\n",
        "    def set_seed(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "       \n",
        "    \n",
        "    def transform_tr(self, sample):                              \n",
        "        geometric_transforms = [\n",
        "            transforms.RandomApply(torch.nn.ModuleList([\n",
        "                                        transforms.RandomCrop((512, 1024)),\n",
        "                                        ]), \n",
        "                                    p=0.2),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "        ]\n",
        "\n",
        "        color_transforms = [\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
        "        ]\n",
        "        \n",
        "        state = torch.get_rng_state()\n",
        "        img = sample[0]\n",
        "        img_transforms = transforms.Compose(geometric_transforms + color_transforms)\n",
        "        img_aug = img_transforms(img)\n",
        "\n",
        "        torch.set_rng_state(state)\n",
        "        mask = sample[1]\n",
        "        mask_transforms = transforms.Compose(geometric_transforms)\n",
        "        mask_aug = mask_transforms(mask)\n",
        "        \n",
        "        sample = (img_aug, mask_aug)\n",
        "\n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_val(self, sample):\n",
        "        \n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def transform_ts(self, sample):\n",
        "\n",
        "        composed_transforms = transforms.Compose([ToTensor()])\n",
        "        return composed_transforms(sample)\n",
        "\n",
        "    def get_img_list(self):\n",
        "        img_list = [(img_path, os.path.join(self.annotations_base,\n",
        "                                            img_path.split(os.sep)[-2],\n",
        "                                            os.path.basename(img_path)[:-15] + 'gtFine_labelIds.png')) for img_path in\n",
        "                    self.files[self.split]]\n",
        "        return img_list\n",
        "\n",
        "    def set_augmentation_type(self):\n",
        "        self.transform_train = self.transform_tr\n",
        "        self.transform_validation = self.transform_val\n",
        "        self.transform_test = self.transform_ts\n",
        "\n",
        "    def reduce_class_map(self):\n",
        "    # Reduce the number of classes from 19 to the following 3: “background”, “vehicle”, “road”:\n",
        "    # The guidelines: \n",
        "    #   1. Everything that can be found on the road wil be marked as vehicle:\n",
        "    #       a. All vechile types.\n",
        "    #       b. Person and rider classes as well, as they can be found on the road, and should not be labeled as \"Road\"\n",
        "\n",
        "    # valid class id  | class name      | new class name    | new class id | \n",
        "    #     0           | unlabeled       |  unlabeld         |  0\n",
        "    #     7           | road            |  road             |  1\n",
        "    #     8           | sidewalk        |  background       |  2\n",
        "    #     11          | building        |  background       |  2\n",
        "    #     12          | wall            |  background       |  2\n",
        "    #     13          | fence           |  background       |  2\n",
        "    #     17          | pole            |  background       |  2\n",
        "    #     19          | traffic light   | background        |  2\n",
        "    #     20          | traffic sign    | background        |  2\n",
        "    #     21          | vegetation      | background        |  2\n",
        "    #     22          | terrain         |  background       |  2\n",
        "    #     23          | sky             |  background       |  2\n",
        "    #     24          | person          |  vehicle          |  3\n",
        "    #     25          | rider           |  vehicle          |  3\n",
        "    #     26          | car             |  vehicle          |  3\n",
        "    #     27          | truck           |  vehicle          |  3\n",
        "    #     28          | bus             |  vehicle          |  3\n",
        "    #     31          | train           |  vehicle          |  3\n",
        "    #     32          | motorcycle      |  vehicle          |  3\n",
        "    #     33          | bicycle         |  vehicle          |  3\n",
        "    \n",
        "        class_map = {0: 0, #unlabeled -> unlabeled\n",
        "                    7: 1, #road -> road\n",
        "                    8: 2, #sidewalk -> background\n",
        "                    11: 2, #building -> background\n",
        "                    12: 2, #wall -> background\n",
        "                    13: 2, #fence -> background\n",
        "                    17: 2, #pole -> background\n",
        "                    19: 2, #traffic light -> background\n",
        "                    20: 2, #traffic sign -> background\n",
        "                    21: 2, #vegetation -> background\n",
        "                    22: 2, #terrain -> background\n",
        "                    23: 2, #sky -> background\n",
        "                    24: 3, #person -> vehicle25\n",
        "                    25: 3, #rider -> vehicle\n",
        "                    26: 3, #car -> vehicle\n",
        "                    27: 3, #truck -> vehicle\n",
        "                    28: 3, #bus -> vehicle\n",
        "                    31: 3, #train -> vehicle\n",
        "                    32: 3, #motorcycle -> vehicle\n",
        "                    33: 3 #bicycle -> vehicle\n",
        "                    }\n",
        "\n",
        "        return class_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb5B-A7n4QoP"
      },
      "source": [
        "### Training Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts0HsEVuFdF8",
        "outputId": "f1430e2b-6f5a-494c-8321-0f7578e48226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unet in /opt/conda/lib/python3.8/site-packages (0.7.7)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from unet) (1.10.0a0+ecc3718)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->unet) (3.10.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cSB4ovx5FDRh"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.transform import resize\n",
        "from unet import UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6dThpmTxdoPV"
      },
      "outputs": [],
      "source": [
        "img_list_dir = \"/DATA/img_list\"\n",
        "# For example: img_list_dir = \"/content/drive/MyDrive/Algo_Home_Assignment/Cityscapes/img_list\"\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tHl1US3jLf",
        "outputId": "d7a6f47a-8f62-42a8-bc0b-2ae5f7bfa625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2975 train images\n",
            "Found 500 val images\n"
          ]
        }
      ],
      "source": [
        "train_set = TrainDataset(img_list_path=img_list_dir, split='train')\n",
        "val_set = TrainDataset(img_list_path=img_list_dir, split='val')\n",
        "dataloader_training = DataLoader(train_set, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_set, batch_size=1, drop_last=True, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config():\n",
        "  def __init__(self) -> None:\n",
        "      self.EPOCHS = 50\n",
        "      # lr\n",
        "      self.lr = 0.001\n",
        "      self.momentum = 0.9\n",
        "      \n",
        "      # lr scheduler\n",
        "      self.multistep_milestones = [30, 45]\n",
        "      self.lr_gamma = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ipywidgets import IntProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "u3LihqMx3nJo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5daf0367fa9e4704887426c277b17447",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Progress'), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80784600ae7041b58538627b259c82d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=371.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected object of scalar type Float but got scalar type Double for argument #3 'weight' in call to _thnn_nll_loss2d_forward",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1410/3175027041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_1410/14089671.py\u001b[0m in \u001b[0;36mrun_trainer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m\"\"\"Training block\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"\"\"Validation block\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_1410/14089671.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zerograd the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# one forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1055\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #3 'weight' in call to _thnn_nll_loss2d_forward"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    print(\"CPU\")\n",
        "    torch.device('cpu')\n",
        "\n",
        "# model\n",
        "model = UNet(in_channels=3,\n",
        "             out_classes=19,\n",
        "             out_channels_first_layer=64,\n",
        "             num_encoding_blocks=2,\n",
        "             padding=2,\n",
        "             dimensions=2).to(device)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ---------------------------------  \"COMPLETE ME\"  ---------------------------\n",
        "config = Config()\n",
        "\n",
        "# Add a loss function \n",
        "org_cityscapes_class_weights = [0.8373, 0.9180, 0.8660, 1.0345, 1.0166, 0.9969, 0.9754,\n",
        "                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037,\n",
        "                        1.0865, 1.0955, 1.0865, 1.1529, 1.0507]\n",
        "\n",
        "class_remapping = {\"0\": 0, \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1, \"6\": 1, \"7\": 1, \"8\": 1, \"9\": 1, \"10\": 1, \"11\": 1, \"12\": 1, \"13\": 2, \"14\": 2, \"15\": 2, \"16\": 2, \"17\": 2, \"18\": 2, \"255\": 255}\n",
        "class_weights = np.zeros(19)\n",
        "for cls_id in class_remapping.keys():\n",
        "    if class_remapping[cls_id] != 255:\n",
        "        class_weights[class_remapping[cls_id]] += org_cityscapes_class_weights[int(cls_id)]\n",
        "\n",
        "#weight more for less frequent classes\n",
        "class_weights[:3] = 1/class_weights[:3]\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float().to(device), ignore_index=255)\n",
        "# criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "\n",
        "## Add an optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
        "\n",
        "\n",
        "## Add a learning rate scheduler \n",
        "lr_scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=config.multistep_milestones, gamma=config.lr_gamma)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=dataloader_training,\n",
        "                  validation_DataLoader=dataloader_validation,\n",
        "                  lr_scheduler=lr_scheduler,\n",
        "                  epochs=config.EPOCHS,\n",
        "                  epoch=0,\n",
        "                  notebook=True)\n",
        "\n",
        "# start training\n",
        "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWtze-dJj5Lc"
      },
      "source": [
        "GOOD LUCK !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VTkzkTvNkVkN",
        "-6WaDZ6C4T-x"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
